{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06202422-5196-48cc-b5ad-999baea2864c",
   "metadata": {},
   "source": [
    "# Interleave headers\n",
    "## 2022-01-16\n",
    "- I first grepped the various headers in the text email files from the command line. \n",
    "- Each line of each files contains the container file, the line number, and the header, and the remaining line items. \n",
    "- Objective is to construct a single file with the headers in a chronological order, and from there construct a list of proper emails. Hopefully taking the threads into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fcf3a20-88b7-4e05-9493-08331df2190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re\n",
    "from datetime import datetime\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d1be4d-7266-422f-b22b-b484cc31a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath=\"./email_txt/downloaded_from_data_tallahassee/\"\n",
    "files = glob(inputpath + \"*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7970ee2d-6cc5-40f8-a2ac-68ed62349644",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dict = {}\n",
    "for file in files[0:5]:\n",
    "    with open(file, \"r\") as f:\n",
    "        files_dict[file] = f.read().splitlines(); # remove \\n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc32a19-c582-488a-8073-791b691f8265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./email_txt/downloaded_from_data_tallahassee/23-11-PCSgmail2014-2017.txt\n",
      "./email_txt/downloaded_from_data_tallahassee/15-6-HunterHarp2014-1.txt\n",
      "./email_txt/downloaded_from_data_tallahassee/17-2-IB2013-1-0.txt\n",
      "./email_txt/downloaded_from_data_tallahassee/34-1-ScottMaddox1-Redacted.txt\n",
      "./email_txt/downloaded_from_data_tallahassee/18-5-Inkbridge2013-3-0.txt\n"
     ]
    }
   ],
   "source": [
    "for k in files_dict.keys(): print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f100b8e-a205-4f65-8129-1154ad1b46a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in files_dict.keys():\n",
    "    # print(len(files_dict[k]))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b8432e-cec5-43f5-9ca2-ed35e4fcdcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = ['date', 'sent', 'to', 'cc', 're', 'bcc', 'when', 'subject', 'original', 'from']\n",
    "folder = \"search_results\"\n",
    "# prefixes = ['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f64932ab-9100-4a30-b05c-8247dcf7e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "breakup_date = re.compile(r'([\\w-]+?.txt):(\\d+?):(.*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "1065043a-d83b-4371-a704-382f151c10c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputpath:  search_results/date_txt\n",
      "inputpath:  search_results/sent_txt\n",
      "inputpath:  search_results/to_txt\n",
      "inputpath:  search_results/cc_txt\n",
      "inputpath:  search_results/re_txt\n",
      "inputpath:  search_results/bcc_txt\n",
      "inputpath:  search_results/when_txt\n",
      "inputpath:  search_results/subject_txt\n",
      "inputpath:  search_results/original_txt\n",
      "inputpath:  search_results/from_txt\n"
     ]
    }
   ],
   "source": [
    "dicts = {}\n",
    "for prefix in prefixes:\n",
    "    inputpath = os.path.join(folder, prefix + \"_txt\")\n",
    "    folder_list = []\n",
    "    dicts[prefix] = folder_list\n",
    "\n",
    "    with open(inputpath, \"r\", encoding=\"utf8\") as f:\n",
    "        print(\"inputpath: \", inputpath)\n",
    "        for line in f.readlines():\n",
    "            filenm, ln, rest = breakup_date.findall(line)[0]\n",
    "            # grep -n numbers lines from 1, and I want 0-based\n",
    "            d = {\"filenm\": filenm, \"ln\": int(ln)-1, \"rest\": rest.rstrip()}\n",
    "            folder_list.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "014e8f5e-09d5-457b-b00c-871a852f81c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date (15465, 3)\n",
      "sent (58640, 3)\n",
      "to (71105, 3)\n",
      "cc (36448, 3)\n",
      "re (33718, 3)\n",
      "bcc (172, 3)\n",
      "when (647, 3)\n",
      "subject (74181, 3)\n",
      "original (1, 3)\n",
      "from (71557, 3)\n"
     ]
    }
   ],
   "source": [
    "dfs = {}\n",
    "for k in dicts.keys():\n",
    "    dfs[k] = pd.DataFrame(dicts[k])\n",
    "    print(k, dfs[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "ad86c6f2-f1c4-4b78-ae39-9bce95bb4c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15465, 3)"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = dfs['date']\n",
    "dates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "29f2e428-446d-4cd9-99ff-da996621e28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenm</th>\n",
       "      <th>ln</th>\n",
       "      <th>rest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-1-Cascade-2013.txt</td>\n",
       "      <td>133</td>\n",
       "      <td>Date: Thursday, July 11, 2013 1:08 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10-1-Cascade-2013.txt</td>\n",
       "      <td>1381</td>\n",
       "      <td>Date: August 20, 2013, 6:24:26 PM EDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-1-Cascade-2013.txt</td>\n",
       "      <td>1722</td>\n",
       "      <td>Date: Tue, Jun 25, 2013 at 11:45 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-1-Cascade-2013.txt</td>\n",
       "      <td>2106</td>\n",
       "      <td>Date: Tue, Jun 25, 2013 at 11:45 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1-Cascade-2013.txt</td>\n",
       "      <td>2127</td>\n",
       "      <td>Date: Tue, Jun 25, 2013 at 11:45 AM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  filenm    ln                                   rest\n",
       "0  10-1-Cascade-2013.txt   133  Date: Thursday, July 11, 2013 1:08 PM\n",
       "1  10-1-Cascade-2013.txt  1381  Date: August 20, 2013, 6:24:26 PM EDT\n",
       "2  10-1-Cascade-2013.txt  1722    Date: Tue, Jun 25, 2013 at 11:45 AM\n",
       "3  10-1-Cascade-2013.txt  2106    Date: Tue, Jun 25, 2013 at 11:45 AM\n",
       "4  10-1-Cascade-2013.txt  2127    Date: Tue, Jun 25, 2013 at 11:45 AM"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "f14fb399-db61-4e44-9ea6-4069f544188a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15465"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = dates['rest'].values\n",
    "len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631e7160-f15c-460b-bc1c-dd7653985ea8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-51262dad62fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rest'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'date:'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\(GMT-05:00\\)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfs' is not defined"
     ]
    }
   ],
   "source": [
    "rest = dfs['date'].loc[:, 'rest'].values\n",
    "rest = [r.strip() for r in rest]\n",
    "rest = [r[5:].strip() if r[0:5].lower() == 'date:' else r.strip() for r in rest]\n",
    "for i, r in enumerate(rest):\n",
    "    rest[i] = re.sub(\"\\(GMT-05:00\\)\", \"\", r)\n",
    "dfs['date']['rest'] = rest\n",
    "len(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "bfe2623b-181e-45c4-be1f-605c0cac3b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['date', 'sent', 'to', 'cc', 're', 'bcc', 'when', 'subject', 'original', 'from'])"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b164d-3d9e-4a05-9440-be0800eb02b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "b8dc6c28-f4da-451e-85c2-69245148b74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58640"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest = dfs['sent'].loc[:, 'rest'].values\n",
    "rest = [r.strip() for r in rest]\n",
    "rest = [r[5:].strip() if r[0:5].lower() == 'sent:' else r.strip() for r in rest]\n",
    "for i, r in enumerate(rest):\n",
    "    rr = re.sub(\"\\(GMT-05:00\\).*\", \"\", r)\n",
    "    rr = re.sub(\"\\(UTC-05:00\\).*\", \"\", rr)\n",
    "    rest[i] = rr\n",
    "dfs['sent']['rest'] = rest\n",
    "len(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3836b-ba5c-4b8b-bd5b-0cea4ab837c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "af394f35-71c0-4537-9b0d-4440dcd3187f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15465, 3), 58640)"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['date'].shape, len(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "1af9c99b-2ea3-40de-b55b-ddf471d3426c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenm</th>\n",
       "      <th>ln</th>\n",
       "      <th>rest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-1-Cascade-2013.txt</td>\n",
       "      <td>133</td>\n",
       "      <td>Thursday, July 11, 2013 1:08 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10-1-Cascade-2013.txt</td>\n",
       "      <td>1381</td>\n",
       "      <td>August 20, 2013, 6:24:26 PM EDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-1-Cascade-2013.txt</td>\n",
       "      <td>1722</td>\n",
       "      <td>Tue, Jun 25, 2013 at 11:45 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-1-Cascade-2013.txt</td>\n",
       "      <td>2106</td>\n",
       "      <td>Tue, Jun 25, 2013 at 11:45 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1-Cascade-2013.txt</td>\n",
       "      <td>2127</td>\n",
       "      <td>Tue, Jun 25, 2013 at 11:45 AM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  filenm    ln                             rest\n",
       "0  10-1-Cascade-2013.txt   133  Thursday, July 11, 2013 1:08 PM\n",
       "1  10-1-Cascade-2013.txt  1381  August 20, 2013, 6:24:26 PM EDT\n",
       "2  10-1-Cascade-2013.txt  1722    Tue, Jun 25, 2013 at 11:45 AM\n",
       "3  10-1-Cascade-2013.txt  2106    Tue, Jun 25, 2013 at 11:45 AM\n",
       "4  10-1-Cascade-2013.txt  2127    Tue, Jun 25, 2013 at 11:45 AM"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "27c83d8d-0f6a-4bb7-b5af-e92ba9c73554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenm</th>\n",
       "      <th>ln</th>\n",
       "      <th>rest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-1-Cascade-2013.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>Sunday, July 14, 2013 2:39 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10-1-Cascade-2013.txt</td>\n",
       "      <td>20</td>\n",
       "      <td>Sunday, July 14, 2013 2:39 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-1-Cascade-2013.txt</td>\n",
       "      <td>39</td>\n",
       "      <td>Monday, July 15, 2013 8:18 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-1-Cascade-2013.txt</td>\n",
       "      <td>192</td>\n",
       "      <td>Monday, July 15, 2013 8:35 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1-Cascade-2013.txt</td>\n",
       "      <td>230</td>\n",
       "      <td>Monday, July 15, 2013 8:35 AM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  filenm   ln                           rest\n",
       "0  10-1-Cascade-2013.txt    1  Sunday, July 14, 2013 2:39 PM\n",
       "1  10-1-Cascade-2013.txt   20  Sunday, July 14, 2013 2:39 PM\n",
       "2  10-1-Cascade-2013.txt   39  Monday, July 15, 2013 8:18 AM\n",
       "3  10-1-Cascade-2013.txt  192  Monday, July 15, 2013 8:35 AM\n",
       "4  10-1-Cascade-2013.txt  230  Monday, July 15, 2013 8:35 AM"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['sent'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "e126f8b4-2bbf-4102-94d5-1bf0ae86cfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Thursday, July 11, 2013 1:08 PM\n",
      "1 August 20, 2013, 6:24:26 PM EDT\n",
      "2 Tue, Jun 25, 2013 at 11:45 AM\n",
      "3 Tue, Jun 25, 2013 at 11:45 AM\n",
      "4 Tue, Jun 25, 2013 at 11:45 AM\n",
      "5 November 19, 2013 at 7:51:38 PM EST\n",
      "6 November 19, 2013 at 7:51:38 PM EST\n",
      "7 November 26, 2013 at 11:06:32 AM EST\n",
      "8 November 26, 2013 at 11:06:32 AM EST\n",
      "9 November 26, 2013 at 11:06:32 AM EST\n",
      "10 December 10, 2013 at 4:32:37 PM EST\n",
      "11 Tue, Dec 10, 2013 2:06 PM\n",
      "12 December 10, 2013 at 4:32:37 PM EST\n",
      "13 Tue, Dec 10, 2013 2:06 PM\n",
      "14 December 10, 2013 at 4:32:37 PM EST\n",
      "15 Tue, Dec 10, 2013 2:06 PM\n",
      "16 December 10, 2013 at 4:32:37 PM EST\n",
      "17 Tue, Dec 10, 2013 2:06 PM\n",
      "18 December 10, 2013 at 4:32:37 PM EST\n",
      "19 Tue, Dec 10, 2013 2:06 PM\n",
      "20 December 10, 2013 at 4:32:37 PM EST\n",
      "21 Tue, Dec 10, 2013 2:06 PM\n",
      "22 December 10, 2013 at 4:32:37 PM EST\n",
      "23 Tue, Dec 10, 2013 2:06 PM\n",
      "24 December 10, 2013 at 4:32:37 PM EST\n",
      "25 Tue, Dec 10, 2013 2:06 PM\n",
      "26 December 10, 2013 at 4:32:37 PM EST\n",
      "27 Tue, Dec 10, 2013 2:06 PM\n",
      "28 December 10, 2013 at 4:32:37 PM EST\n",
      "29 Tue, Dec 10, 2013 2:06 PM\n",
      "30 December 5, 2013\n",
      "31 December 10, 2013 at 4:32:37 PM EST\n",
      "32 Tue, Dec 10, 2013 2:06 PM\n",
      "33 December 5, 2013\n",
      "34 December 10, 2013 at 4:32:37 PM EST\n",
      "35 Tue, Dec 10, 2013 2:06 PM\n",
      "36 December 5, 2013\n",
      "37 December 10, 2013 at 4:32:37 PM EST\n",
      "38 Tue, Dec 10, 2013 2:06 PM\n",
      "39 December 5, 2013\n",
      "40 December 13, 2013, 4:46:58 PM EST\n",
      "41 February 7, 2014 at 11:42:14 AM EST\n",
      "42 February 7, 2014 at 11:42:14 AM EST\n",
      "43 February 7, 2014 at 11:42:14 AM EST\n",
      "44 March 14, 2014 at 6:36:53 PM EDT\n",
      "45 March 14, 2014 at 6:36:53 PM EDT\n",
      "46 Tue, Mar 18, 2014 at 5:03 PM\n",
      "47 Tue, Mar 18, 2014 at 5:03 PM\n",
      "48 FEB 6 2014 45-5577241\n"
     ]
    }
   ],
   "source": [
    "for i in range(49):\n",
    "    print(i, dfs['date'].loc[i, 'rest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "65b1a248-cc76-4357-81a6-1318d3df5db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erlebach/opt/anaconda3/envs/torch/lib/python3.8/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname PDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/Users/erlebach/opt/anaconda3/envs/torch/lib/python3.8/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname CST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/Users/erlebach/opt/anaconda3/envs/torch/lib/python3.8/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname PMEDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/Users/erlebach/opt/anaconda3/envs/torch/lib/python3.8/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname MDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/Users/erlebach/opt/anaconda3/envs/torch/lib/python3.8/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname CDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/Users/erlebach/opt/anaconda3/envs/torch/lib/python3.8/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname HST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "/Users/erlebach/opt/anaconda3/envs/torch/lib/python3.8/site-packages/dateutil/parser/_parser.py:1207: UnknownTimezoneWarning: tzname PST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "dates1 = pd.to_datetime(dfs['date'].loc[:, 'rest'], errors='coerce');\n",
    "# lines of data_indexes translated to NaT, so are not valid date-times\n",
    "NaT_date_indexes = dates1[dates1.isna() == True].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "c6987f90-0a57-4b6d-b4ad-07e24d83e8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15465,)"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d037d20-696f-4f04-b815-11338d98144c",
   "metadata": {},
   "source": [
    "d = dates.iloc[NaT_date_indexes,:]\n",
    "for i in range(1000):\n",
    "    print(d.iloc[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "1eafe2c5-bffe-4fa8-90a4-c0f5a307b634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244,)"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = pd.to_datetime(dfs['sent'].loc[:, 'rest'], exact=False, errors='coerce');\n",
    "# lines of data_indexes translated to NaT, so are not valid date-times\n",
    "NaT_sent_indexes = dates1[sent1.isna() == True].index\n",
    "sent_NaT = dfs['sent'].iloc[NaT_sent_indexes]\n",
    "NaT_sent_indexes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "dd41060f-af25-4785-98ca-bb987cd9b169",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count=  56\n",
      "0                    NaT\n",
      "1                    NaT\n",
      "2                    NaT\n",
      "3    2016-02-21 13:54:00\n",
      "4    2016-02-21 13:54:00\n",
      "5    2016-02-21 13:54:00\n",
      "6    2016-02-21 13:54:00\n",
      "7    2017-01-13 16:07:00\n",
      "8    2017-01-13 15:16:00\n",
      "9    2017-01-13 11:53:00\n",
      "10                   NaT\n",
      "11                   NaT\n",
      "12   2017-01-10 16:36:00\n",
      "13   2017-01-10 15:13:00\n",
      "14   2017-01-10 16:36:00\n",
      "15   2017-01-10 15:13:00\n",
      "16   2017-01-13 16:07:00\n",
      "17   2017-01-13 15:16:00\n",
      "18   2017-01-13 11:53:00\n",
      "19                   NaT\n",
      "20                   NaT\n",
      "21   2017-01-10 16:36:00\n",
      "22   2017-01-10 15:13:00\n",
      "23   2017-01-10 16:36:00\n",
      "24   2017-01-10 15:13:00\n",
      "25   2012-12-20 13:26:00\n",
      "26   2012-12-20 13:16:00\n",
      "27   2012-12-20 10:54:00\n",
      "28   2012-12-20 13:26:00\n",
      "29   2012-12-20 13:16:00\n",
      "30   2012-12-20 10:54:00\n",
      "31                   NaT\n",
      "32                   NaT\n",
      "33   2014-08-26 15:07:00\n",
      "34   2012-12-20 14:03:00\n",
      "35   2012-12-20 15:15:00\n",
      "36   2012-12-20 14:03:00\n",
      "37   2012-12-20 14:03:00\n",
      "38   2012-12-20 13:26:00\n",
      "39   2012-12-20 13:16:00\n",
      "40   2012-12-20 10:54:00\n",
      "41   2012-12-20 13:26:00\n",
      "42   2012-12-20 13:16:00\n",
      "43   2012-12-20 10:54:00\n",
      "44   2013-06-14 15:02:00\n",
      "45   2013-06-14 15:02:00\n",
      "46   2012-12-20 14:03:00\n",
      "47   2012-12-20 15:15:00\n",
      "48   2012-12-20 14:03:00\n",
      "49   2012-12-20 14:03:00\n",
      "50   2012-12-20 13:26:00\n",
      "51   2012-12-20 13:16:00\n",
      "52   2012-12-20 10:54:00\n",
      "53   2012-12-20 13:26:00\n",
      "54   2012-12-20 13:16:00\n",
      "55   2012-12-20 10:54:00\n",
      "Name: col, dtype: datetime64[ns]\n",
      "                                                  col\n",
      "0                Monday, December 15, 2014 10:28 AM *\n",
      "1                Monday, December 15, 2014 10:28 AM *\n",
      "2                Monday, December 15, 2014 10:28 AM *\n",
      "3                  Sunday, February 21, 2016 1:54 PM \n",
      "4                  Sunday, February 21, 2016 1:54 PM \n",
      "5                Sunday, February   21, 2016 1:54 PM \n",
      "6                Sunday, February   21, 2016 1:54 PM \n",
      "7                 Friday, January 13, 2017 4:07 PM   \n",
      "8                   Friday, January 13, 2017 3:16 PM \n",
      "9                  Friday, January 13, 2017 11:53 AM \n",
      "10                T uesday, January 10, 2017 4:36 PM \n",
      "11                T uesday, January 10, 2017 3:13 PM \n",
      "12                 Tuesday, January 10, 2017 4:36 PM \n",
      "13                 Tuesday, January 10, 2017 3:13 PM \n",
      "14                 Tuesday, January 10, 2017 4:36 PM \n",
      "15                 Tuesday, January 10, 2017 3:13 PM \n",
      "16                Friday, January 13, 2017 4:07 PM   \n",
      "17                  Friday, January 13, 2017 3:16 PM \n",
      "18                 Friday, January 13, 2017 11:53 AM \n",
      "19                T uesday, January 10, 2017 4:36 PM \n",
      "20                T uesday, January 10, 2017 3:13 PM \n",
      "21                 Tuesday, January 10, 2017 4:36 PM \n",
      "22                 Tuesday, January 10, 2017 3:13 PM \n",
      "23                 Tuesday, January 10, 2017 4:36 PM \n",
      "24                 Tuesday, January 10, 2017 3:13 PM \n",
      "25               Thursday, December 20, 2012 1:26 PM \n",
      "26               Thursday, December 20, 2012 1:16 PM \n",
      "27            Thursday,   December 20, 2012 10:54 AM \n",
      "28               Thursday, December 20, 2012 1:26 PM \n",
      "29               Thursday, December 20, 2012 1:16 PM \n",
      "30            Thursday,   December 20, 2012 10:54 AM \n",
      "31   Thursday, April 09, 2015 06:04 AM US Mountain...\n",
      "32   Thursday, April 09, 2015 06:04 AM US Mountain...\n",
      "33                 Tuesday, August 26, 2014 3:07 PM  \n",
      "34              Thursday, December 20, 2012 2:03 PM  \n",
      "35               Thursday, December 20, 2012 3:15 PM \n",
      "36              Thursday, December 20, 2012 2:03 PM  \n",
      "37              Thursday, December 20, 2012 2:03 PM  \n",
      "38               Thursday, December 20, 2012 1:26 PM \n",
      "39               Thursday, December 20, 2012 1:16 PM \n",
      "40            Thursday,   December 20, 2012 10:54 AM \n",
      "41               Thursday, December 20, 2012 1:26 PM \n",
      "42               Thursday, December 20, 2012 1:16 PM \n",
      "43            Thursday,   December 20, 2012 10:54 AM \n",
      "44                     Friday, June 14, 2013 3:02 PM \n",
      "45                     Friday, June 14, 2013 3:02 PM \n",
      "46              Thursday, December 20, 2012 2:03 PM  \n",
      "47               Thursday, December 20, 2012 3:15 PM \n",
      "48              Thursday, December 20, 2012 2:03 PM  \n",
      "49              Thursday, December 20, 2012 2:03 PM  \n",
      "50               Thursday, December 20, 2012 1:26 PM \n",
      "51               Thursday, December 20, 2012 1:16 PM \n",
      "52            Thursday,   December 20, 2012 10:54 AM \n",
      "53               Thursday, December 20, 2012 1:26 PM \n",
      "54               Thursday, December 20, 2012 1:16 PM \n",
      "55            Thursday,   December 20, 2012 10:54 AM \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n31   Thursday, April 09, 2015 06:04 AM US Mountain...\\n32   Thursday, April 09, 2015 06:04 AM US Mountain...\\nPerhaps add a column of time corrections? mostly 0, but +2 hours if US Mountain ...? \\n'"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most errors start with >Sent, which are not original emails, but rather, Re, Fwd, or emails inside threads. \n",
    "# So we can remove them. On the other side, \n",
    "# sent_NaT.head(50)\n",
    "count = 0\n",
    "inputpath=\"./email_txt/downloaded_from_data_tallahassee/\"\n",
    "\n",
    "rows = []\n",
    "for row in sent_NaT.iterrows():\n",
    "    # Only print the lines that do not start with >\n",
    "    # if sent_NaT.loc[i, 'rest'].value != '>':a\n",
    "    ln = int(row[1][1])\n",
    "    fn = row[1][0]\n",
    "    rest = row[1][2]\n",
    "    # print(\"fn: \", fn)\n",
    "    # print(\"inputpath: \", inputpath)\n",
    "    # print(\"files_dict.keys(): \", files_dict.keys())\n",
    "    # lines = files_dict[inputpath+fn]\n",
    "    # print(\"lines: \", lines[ln : ln+1], \"\\n\")\n",
    "    # print(\"ln: \", ln)\n",
    "    if row[1][2][0] != '>':\n",
    "        lines = files_dict[inputpath+fn]\n",
    "        # Took care of zero-based line numbers earlier in this file\n",
    "        # print(\"lines: \", lines[ln : ln+2])\n",
    "        # print(fn, ln, rest)\n",
    "        # print()\n",
    "        Lines = \" \".join(lines[ln:ln+10])\n",
    "        # Remove \"Sent:\" if it occurs at the front\n",
    "        Lines = re.sub(\"\\b.*?Sent:\", \"\", Lines)\n",
    "        # Remove leading \">\" denoting threads\n",
    "        Lines = re.sub(\">+? Sent:\", \"\", Lines)\n",
    "        # Remove from To: onward \n",
    "        Lines = re.sub(\"(.*?)(To:.*)\", r\"\\1\", Lines)\n",
    "        Lines = re.sub(\"\\*\\*\", \"\", Lines)\n",
    "        # Fix \"...Tuesday August 26:\" into \"...Tuesdsay August 26, \"...\n",
    "        Lines = re.sub(\"([a-z] [0-9]*?):\", r\"\\1, \", Lines)\n",
    "        # Replace \": \" by \":\" to handle bad time\n",
    "        Lines = re.sub(\": \", \":\", Lines)\n",
    "\n",
    "        # print(\"row: \", row)\n",
    "        rows.append(Lines)\n",
    "        # print(\"Lines: \", Lines)\n",
    "        count += 1\n",
    "        #print(\"-------------------------\")\n",
    "    #( print(sent_NaT.loc[i, 'rest'])\n",
    "print(\"count= \", count)\n",
    "ddf = pd.DataFrame({'col':rows})\n",
    "datess = pd.to_datetime(ddf['col'], exact=False, errors='coerce')\n",
    "print(datess)\n",
    "print(ddf)\n",
    "\"\"\"\n",
    "31   Thursday, April 09, 2015 06:04 AM US Mountain...\n",
    "32   Thursday, April 09, 2015 06:04 AM US Mountain...\n",
    "Perhaps add a column of time corrections? mostly 0, but +2 hours if US Mountain ...? \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e52b3-22ef-40d6-bce0-06453c947774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8406877-aad5-439a-8e47-b768bc5259c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a9114-bad6-4fa0-8172-a70c53aa0532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
